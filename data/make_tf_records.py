# -*- coding: utf-8 -*-
"""make_tf_records.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15FjmQPpakIYtX8nmrwa9uiQo1PvkNvF4
"""
from __future__ import division

import glob
import os
import random
import shutil

import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
from nvidia.dali.pipeline import Pipeline
import nvidia.dali.ops as ops
import nvidia.dali.types as types
import tensorflow as tf


def show_images(image_batch, batch_size):
    columns = 4
    rows = (batch_size + 1) // (columns)
    fig = plt.figure(figsize = (32,(32 // columns) * rows))
    gs = gridspec.GridSpec(rows, columns)
    for j in range(rows*columns):
        plt.subplot(gs[j])
        plt.axis("off")
        plt.imshow(image_batch.at(j))

def _int64_feature(value):
  if not isinstance(value, list):
    value = [value]
  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _convert_to_example(filename, image_buffer, height, width):
  example = tf.train.Example(features=tf.train.Features(feature={
      'image/height': _int64_feature(height),
      'image/width': _int64_feature(width),
      'image/filename': _bytes_feature(os.path.basename(filename)),
      'image/encoded': _bytes_feature(image_buffer)}))
  return example

def image_parser(filename, target_size=480):
  image_string = tf.read_file(filename)
  image = tf.image.decode_jpeg(image_string, 3)
  height = tf.shape(image)[0]
  width = tf.shape(image)[1]
  min_dim = tf.math.minimum(width, height)
  ratio = tf.to_float(min_dim) / tf.constant(target_size, dtype=tf.float32)
  new_height = tf.to_int32(tf.to_float(height) / ratio)
  new_width = tf.to_int32(tf.to_float(width) / ratio)
  image = tf.image.resize_images(image, [new_height, new_width])
  height, width = tf.shape(image)[0], tf.shape(image)[1]
  image = tf.cast(image, tf.uint8)
  image = tf.image.encode_jpeg(image, format='rgb', quality=70)
  return filename, image, height, width

def make_dataset_iterator(filenames):
  dataset = tf.data.Dataset.from_tensor_slices(filenames)
  dataset = dataset.map(image_parser, num_parallel_calls=4).batch(1, drop_remainder=True).prefetch(8)
  iterator_next = dataset.make_one_shot_iterator().get_next()
  return iterator_next

class ImageCoder():

  def __init__(self, filenames):
    self._iterator = make_dataset_iterator(filenames)
    self._sess = tf.Session()

  def get_next(self, filename):
    try:
        res = self._sess.run(self._iterator)
    except:
        print('Error in decoding file: ', filename)
        return filename, None, 0, 0
    return [e[0] for e in res]

def process_image_batch(coder, output_file, filenames):
  writer = tf.python_io.TFRecordWriter(output_file)
  for i, filename in enumerate(filenames):
    if i % 100 == 0:
      print(i, ':', filename)
    filename, image_buffer, height, width = coder.get_next(filename)
    if image_buffer is not None:
      example = _convert_to_example(filename, image_buffer, height, width)
      writer.write(example.SerializeToString())
  writer.close()

def _check_or_create_dir(directory):
  """Check if directory exists otherwise create it."""
  if not tf.gfile.Exists(directory):
    tf.gfile.MakeDirs(directory)

def process_dataset(filenames, output_directory, prefix, num_shards):

  chunk_size = int(len(filenames) / num_shards)
  coder = ImageCoder(filenames)
  files = []
  for shard in range(num_shards):
    chunk_files = filenames[shard*chunk_size: (shard+1)*chunk_size]
    output_file = os.path.join(
        output_directory, '%s-%.5d-of-%.5d' % (prefix, shard, num_shards))
    process_image_batch(coder, output_file, chunk_files)
    tf.logging.info('Finished writing file: %s' % output_file)
    files.append(output_file)
  return files

def get_corrupted_files(filenames, coder):
  corrupted_filenames = []
  for filename in filenames:
    with tf.gfile.FastGFile(filename, 'rb') as f:
      img_data = f.read()
    try:
      print(filename)
      image = coder.decode_jpeg(img_data)
    except:
      corrupted_filenames.append(filename)
  return corrupted_filenames

class WikiartPipeline(Pipeline):
    def __init__(self, batch_size, num_threads, device_id, image_dir="wikiart"):
        super().__init__(batch_size, num_threads, device_id, seed = 12)
        self.input = ops.FileReader(file_root = image_dir, initial_fill = 21)
        self.decode = ops.HostDecoder(output_type = types.RGB)
        self.resize = ops.Resize(device = "gpu", resize_shorter = 480)

    def define_graph(self):
        jpegs, labels = self.input()
        images = self.decode(jpegs)
        resized_images = self.resize(images.gpu())
        return (resized_images, labels)

random.seed(0)
def make_shuffle_idx(n):
  order = list(range(n))
  random.shuffle(order)
  return order

image_dir = "style_images/styles"
batch_size = 16
filenames = tf.gfile.Glob('contents/images')

if False:
  coder = ImageCoder()
  corrupted_files = get_corrupted_files(filenames, coder)
  for corrupted_file in corrupted_files:
    os.remove(corrupted_file)
  filenames = [f for f in filenames if f not in corrupted_file]

training_files = tf.gfile.Glob(os.path.join('contents', 'images', '*'))
idx = make_shuffle_idx(len(training_files))
training_files = [training_files[i].encode() for i in idx]

process_dataset(training_files, 'data_records', 'content_test', 1)

