# -*- coding: utf-8 -*-
"""style_transfer_adain_tpu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K0caWAi1NTM_fLdwQiJFyGAKSfvsEgdP
"""

# !gsutil -m rm -r 'gs://tf-model-dir/style_transfer_dir/'
# !gsutil -m rm -r 'gs://tf-model-dir/style_transfer_adain_like_dir/'
# !gsutil -m rm -r gs://tf-model-dir/style_transfer_adaptive_instance_like_dir/
# !gsutil ls gs://coco-tfrecords/autoencoder_dir/

if False:
  !git clone https://github.com/davidpham87/style_transfer.git
  !rm -r style_transfer/data/contents/sequences/
  !rm style_transfer/data/style_images/styles/pencil.jpg
  !rm style_transfer/data/style_images/styles/escher_sphere.jpg

load_drive_and_gce = False

if load_drive_and_gce:

  from google.colab import drive
  drive.mount('/content/drive')

  from google.colab import auth
  auth.authenticate_user()

  !gcloud config set project neural-art
  # !gsutil cp -r drive/My\ Drive/style_transfer/vgg19_encoder/ gs://coco-tfrecords/

import argparse
import glob
import json
import os
import random
import time

import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import numpy as np
import scipy.misc
import tensorflow as tf

from PIL import Image

from tensorflow import keras
from tensorflow.keras import models
from tensorflow.keras import utils as keras_utils
from tensorflow.keras.applications import vgg19
from tensorflow.keras.layers import UpSampling2D, Input, Conv2D, Conv2DTranspose
from tensorflow.python.estimator import estimator


if 'COLAB_TPU_ADDR' in os.environ:
  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])

  # Upload credentials to TPU.
  with tf.Session(TF_MASTER) as sess:
    with open('/content/adc.json', 'r') as f:
      auth_info = json.load(f)
    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)
  # Now credentials are set for all future sessions on this TPU.
else:
  TF_MASTER=''

# Image processing
def save_image(image, path):
    scipy.misc.imsave(path, image)

import numpy as np

HEIGHT, WIDTH = (256, 256)
BATCH_SIZE = 4

standardize = lambda x: np.array(x)/np.sum(x)

loss_weights = {'content': 1, 'style': 1e3, 'total_variation': 1e-8}
style_loss_weights = {'block1_conv1': 0.5, 'block2_conv1': 0.4, 'block3_conv1':0.3}
params_estimator = {'loss_weights': loss_weights, 'style_loss_weights': style_loss_weights}

train_steps = 2*1e5
num_shards = 8
batch_size_tpu = 8*BATCH_SIZE

# train_steps = int(1e4)
train_steps_per_eval = 4*int(1e3)
iterations_per_loop = 200

COCO_PATH = '../data/data_records/coco*'
WIKI_PATH = '../data/data_records/wiki*'

COCO_PATH = 'drive/My Drive/style_transfer/coco/data_records/coco*'
WIKI_PATH = 'drive/My Drive/style_transfer/wikiart/wiki*'

COCO_PATH = 'gs://coco-tfrecords/coco*'
WIKI_PATH = 'gs://coco-tfrecords/wiki*'

vgg19_variables = [
 'vgg19_encoder/block1_conv1/kernel:0',
 'vgg19_encoder/block1_conv1/bias:0',
 'vgg19_encoder/block1_conv2/kernel:0',
 'vgg19_encoder/block1_conv2/bias:0',
 'vgg19_encoder/block2_conv1/kernel:0',
 'vgg19_encoder/block2_conv1/bias:0',
 'vgg19_encoder/block2_conv2/kernel:0',
 'vgg19_encoder/block2_conv2/bias:0',
 'vgg19_encoder/block3_conv1/kernel:0',
 'vgg19_encoder/block3_conv1/bias:0',
 'vgg19_decoder/block3_conv1_decoder/kernel:0',
 'vgg19_decoder/block3_conv1_decoder/bias:0',
 'vgg19_decoder/block2_conv2_transpose_decoder/kernel:0',
 'vgg19_decoder/block2_conv2_transpose_decoder/bias:0',
 'vgg19_decoder/block2_conv1_decoder/kernel:0',
 'vgg19_decoder/block2_conv1_decoder/bias:0',
 'vgg19_decoder/block1_conv2_transpose_decoder/kernel:0',
 'vgg19_decoder/block1_conv2_transpose_decoder/bias:0',
 'vgg19_decoder/block1_conv1_decoder/kernel:0',
 'vgg19_decoder/block1_conv1_decoder/bias:0',
 'vgg19_decoder/block0_conv1/kernel:0',
 'vgg19_decoder/block0_conv1/bias:0']


linear_style_transfer_variables = [
 'vgg19_encoder/block1_conv1/kernel:0',
 'vgg19_encoder/block1_conv1/bias:0',
 'vgg19_encoder/block1_conv2/kernel:0',
 'vgg19_encoder/block1_conv2/bias:0',
 'vgg19_encoder/block2_conv1/kernel:0',
 'vgg19_encoder/block2_conv1/bias:0',
 'vgg19_encoder/block2_conv2/kernel:0',
 'vgg19_encoder/block2_conv2/bias:0',
 'vgg19_encoder/block3_conv1/kernel:0',
 'vgg19_encoder/block3_conv1/bias:0',
 'vgg19_decoder/block3_conv1_decoder/kernel:0',
 'vgg19_decoder/block3_conv1_decoder/bias:0',
 'vgg19_decoder/block2_conv2_transpose_decoder/kernel:0',
 'vgg19_decoder/block2_conv2_transpose_decoder/bias:0',
 'vgg19_decoder/block2_conv1_decoder/kernel:0',
 'vgg19_decoder/block2_conv1_decoder/bias:0',
 'vgg19_decoder/block1_conv2_transpose_decoder/kernel:0',
 'vgg19_decoder/block1_conv2_transpose_decoder/bias:0',
 'vgg19_decoder/block1_conv1_decoder/kernel:0',
 'vgg19_decoder/block1_conv1_decoder/bias:0',
 'vgg19_decoder/block0_conv1/kernel:0',
 'vgg19_decoder/block0_conv1/bias:0']

warm_start_settings = tf.estimator.WarmStartSettings(
     ckpt_to_initialize_from='gs://tf-model-dir/style_transfer_dir/',
     vars_to_warm_start=linear_style_transfer_variables)
# warm_start_settings = None

# from input_dataset import content_dataset, style_dataset
# from utils import show_images, deprocess_image

def deprocess_image(x):
    x = x.copy()
    # Remove zero-center by mean pixel
    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68
    # 'BGR'->'RGB'
    x = x[:, :, ::-1]
    x = np.clip(x, 0, 255).astype('uint8')
    return x


def reset_tf():
    tf.reset_default_graph()
    tf.keras.backend.clear_session()


def vgg19_encoder(x, scope='vgg19_encoder', trainable=False):
    outputs = {}
    layers = tf.layers
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):

        # Block 1
        x = layers.Conv2D(
            64, (3, 3), activation='relu', padding='same',
            name='block1_conv1', trainable=trainable)(x)
        outputs['block1_conv1'] = x
        x = layers.Conv2D(
            64, (3, 3), activation='relu', padding='same',
            name='block1_conv2', trainable=trainable)(x)
        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

        # Block 2
        x = layers.Conv2D(
            128, (3, 3), activation='relu', padding='same',
            name='block2_conv1', trainable=trainable)(x)
        outputs['block2_conv1'] = x
        x = layers.Conv2D(
            128, (3, 3), activation='relu', padding='same',
            name='block2_conv2', trainable=trainable)(x)
        x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

        # Block 3
        x = layers.Conv2D(
            256, (3, 3), activation='relu', padding='same',
            name='block3_conv1', trainable=trainable)(x)
        outputs['block3_conv1'] = x

        # x = layers.Conv2D(256, (3, 3),
        #               activation='relu',
        #               padding='same',
        #               name='block3_conv2')(x)
        # x = layers.Conv2D(256, (3, 3),
        #               activation='relu',
        #               padding='same',
        #               name='block3_conv3')(x)
        # x = layers.Conv2D(256, (3, 3),
        #               activation='relu',
        #               padding='same',
        #               name='block3_conv4')(x)
        # x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

        # # Block 4
        # x = layers.Conv2D(512, (3, 3), activation='relu',
        #               padding='same', name='block4_conv1')(x)
        # outputs['block4_conv1'] = x
    return outputs


def fixed_padding(inputs, kernel_size=3, data_format='channels_last'):
  """Pads the input along the spatial dimensions independently of input size.
  Args:
    inputs: `Tensor` of size `[batch, channels, height, width]` or
        `[batch, height, width, channels]` depending on `data_format`.
    kernel_size: `int` kernel size to be used for `conv2d` or max_pool2d`
        operations. Should be a positive integer.
    data_format: `str` either "channels_first" for `[batch, channels, height,
        width]` or "channels_last for `[batch, height, width, channels]`.
  Returns:
    A padded `Tensor` of the same `data_format` with size either intact
    (if `kernel_size == 1`) or padded (if `kernel_size > 1`).
  """
  pad_total = kernel_size - 1
  pad_beg = pad_total // 2
  pad_end = pad_total - pad_beg
  if data_format == 'channels_first':
    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],
                                    [pad_beg, pad_end], [pad_beg, pad_end]])
  else:
    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],
                                    [pad_beg, pad_end], [0, 0]])

  return padded_inputs


def padding_layer(x, name='padding'):
  return fixed_padding(x)


def _conv2d(filters, name, padding='valid', trainable=True):
  return tf.layers.Conv2D(
      filters, 3, 1, activation='relu', padding=padding, name=name, trainable=trainable)

def vgg19_decoder(encoding, input_layer_vgg_name='block3_conv1', scope='vgg19_decoder', trainable=False):
    # dimension starts at height/8 width/8 from the original image for block4_conv1
    # dimension starts at height/4 width/4 from the original image for block3_conv1
    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
        x = encoding

        if input_layer_vgg_name == 'block4_conv1':
          x = padding_layer(x)
          x = _conv2d(
            512, name='block4_conv1_decoder', trainble=trainable)(x)
          x = tf.layers.Conv2DTranspose(
            256, 3, 2, activation='relu', padding='same',
            name='block3_conv3_transpose_decoder', trainable=trainable)(x) # h/4 w/4
          x = padding_layer(x)
          x = _conv2d(
            256, name='block3_conv2_decoder', trainable=trainable)(x)

        x = padding_layer(x)
        x = _conv2d(
            256, name='block3_conv1_decoder', trainable=trainable)(x)
        x = tf.layers.Conv2DTranspose(
            256, 3, 2, activation='relu', padding='same',
            name='block2_conv2_transpose_decoder', trainable=trainable)(x) # h/2 w/2
        x = padding_layer(x)
        x = _conv2d(
            128, name='block2_conv1_decoder', trainable=trainable)(x)
        x = Conv2DTranspose(
            128, 3, 2, activation='relu', padding='same',
            name='block1_conv2_transpose_decoder', trainable=trainable)(x)  # h w
        x = _conv2d(64, name='block1_conv1_decoder', padding='same',
            trainable=trainable)(x)
        x = tf.layers.Conv2D(
            3, 1, padding='valid', activation='tanh',
            name='block0_conv1', trainable=trainable)(x)
        output_image = 150*x
    return output_image


def build_vgg19_model(input_shape, weights=None, scope='vgg19_encoder'):

    img_input = keras.layers.Input(shape=input_shape)
    outputs = vgg19_encoder(img_input, scope)
    model = keras.models.Model(
        img_input, [outputs[s] for s in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']],
        name='vgg19_encoder')
    if weights:
        WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'
                       'releases/download/v0.1/'
                       'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')
        weights_path = keras_utils.get_file(
            'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',
            WEIGHTS_PATH_NO_TOP,
            cache_subdir='models',
            file_hash='253f8cb515780f3b799900260a226db6')
        model.load_weights(weights_path, by_name=True)
    return model

def export_vgg19_weights(path='vgg19_encoder/weights.ckpt'):
    encoder = build_vgg19_model((None, None, 3), 'imagenet')
    sess = keras.backend.get_session()
    vgg19_encoder_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='vgg19_encoder')
    tf.train.Saver(vgg19_encoder_variables).save(sess, path)
    return path


def gram_matrix(feature_maps, to_reshape=True):
  """Computes the Gram matrix for a set of feature maps."""
  if to_reshape:
    batch_size, height, width, channels = tf.unstack(tf.shape(feature_maps))
    denominator = tf.to_float(height * width) + 1e-16
    feature_maps = tf.reshape(
        feature_maps, tf.stack([batch_size, height * width, channels]))
  else:
    batch_size, height_width, channels = tf.unstack(tf.shape(feature_maps))
    denominator = tf.to_float(height_width) + 1e-16
  matrix = tf.matmul(feature_maps, feature_maps, adjoint_a=True)
  return matrix / denominator

def compute_content_loss(image, image_decoded):
  return tf.reduce_mean(tf.square(image - image_decoded))


def compute_style_loss(style_map, combination_map, style_loss_per_layer_weights=None):
  s_gram = gram_matrix(style_map) * 1e-3
  c_gram = gram_matrix(combination_map) * 1e-3
  s_dims = tf.shape(style_map)
  denominator = tf.cast(s_dims[0]*s_dims[1]*s_dims[2], tf.float32)
  style_loss = tf.reduce_sum(tf.square(s_gram - c_gram)) / denominator
  return style_loss


def adaptive_instance_normalization(content_embedding, style_embedding, alpha):

    style_mean, style_variance = tf.nn.moments(
        style_embedding, [1, 2], keep_dims=True)
    content_mean, content_variance = tf.nn.moments(
        content_embedding, [1, 2], keep_dims=True)
    epsilon = 1e-12
    normalized_content_features = tf.nn.batch_normalization(
        content_embedding, content_mean, content_variance,
        style_mean, tf.sqrt(style_variance), epsilon)

    alpha_broadcast = tf.expand_dims(tf.expand_dims(alpha, -1), -1)
    normalized_content_embedding = (
         alpha_broadcast * normalized_content_features +
        (1-alpha_broadcast) * content_embedding)

    return normalized_content_embedding

def build_network_adain_model_fn(content_images, style_images):

  content_images_dims = tf.shape(content_images)

  content_encodings = vgg19_encoder(content_images)
  style_encodings = vgg19_encoder(style_images)

  content_style_encodings = adaptive_instance_normalization(
    content_encodings['block3_conv1'],
    style_encodings['block3_conv1'],
    1.0)

  content_style_decodeds = vgg19_decoder(content_style_encodings, trainable=True)
  content_style_recoded = vgg19_encoder(content_style_decodeds)

  return content_style_decodeds, content_encodings, style_encodings, content_style_recoded


def adain_style_transfer_model_fn(features, labels, mode, params):
  """Constructs DCGAN from individual generator and discriminator networks."""
  del labels    # Unconditional GAN does not use labels

  content_images = features['content_images']
  style_images = features['style_images']

  network_outputs = build_network_adain_model_fn(content_images, style_images)

  content_style_decodeds = network_outputs[0]
  content_encodings = network_outputs[1]
  style_encodings = network_outputs[2]
  content_style_recoded = network_outputs[3]

  if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = {
        'generated_images': content_style_decodeds
    }

    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions)

  # Use params['batch_size'] for the batch size inside model_fn
  # batch_size = params['batch_size']   # pylint: disable=unused-variable
  is_training = (mode == tf.estimator.ModeKeys.TRAIN)
  generated_images = content_style_decodeds

  # Calculate generator loss

  # content_loss = compute_content_loss(content_images, content_style_decodeds)
  # total_loss = content_loss

  content_style_recoded = vgg19_encoder(content_style_decodeds)

  loss_weights = {'content': 1, 'style': 1000, 'total_variation': 1e-9}
  loss_weights.update(params.get('loss_weights', {}))

  style_loss_weights = {'block1_conv1': 0.65, 'block2_conv1': 0.3,
                        'block3_conv1': 0.05}
  style_loss_weights.update(params.get('style_loss_weights', {}))

  content_loss = loss_weights['content'] * compute_content_loss(
      content_encodings['block3_conv1'], content_style_recoded['block3_conv1'])
  style_loss = []
  for k in ['block1_conv1', 'block2_conv1', 'block3_conv1']:
    sl = compute_style_loss(style_encodings[k], content_style_recoded[k])
    style_loss.append(style_loss_weights[k]*sl)
  style_loss = loss_weights['style']*tf.reduce_sum(style_loss)
  total_variation_loss = loss_weights['total_variation'] * tf.reduce_mean(
      tf.image.total_variation(content_style_decodeds))

  total_loss = content_loss + style_loss + total_variation_loss

  if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)
    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)
    opt_step = optimizer.minimize(
        total_loss, tf.train.get_global_step(),
        var_list=tf.get_collection(
            tf.GraphKeys.TRAINABLE_VARIABLES))
    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=total_loss, train_op=opt_step)

  elif mode == tf.estimator.ModeKeys.EVAL:
    def _eval_metric_fn(content_loss, style_loss, total_variation_loss):
      # When using TPUs, this function is run on a different machine than the
      # rest of the model_fn and should not capture any Tensors defined there
      metrics = {
          'content': tf.metrics.mean(content_loss),
          'style': tf.metrics.mean(style_loss), 
          'total_variation' : tf.metrics.mean(total_variation_loss)}
      return metrics

    return tf.contrib.tpu.TPUEstimatorSpec(
        mode=mode,
        loss=total_loss,
        eval_metrics=(_eval_metric_fn, [content_loss, style_loss, total_variation_loss]))

  # Should never reach here
  raise ValueError('Invalid mode provided to model_fn')


def content_parser(serialized_example, crop_and_resize=True, image_output_shape=(HEIGHT, WIDTH)):
    """Parses a single tf.Example into image and label tensors."""
    features = tf.parse_single_example(
        serialized_example,
        features={
            "image/encoded": tf.FixedLenFeature([], tf.string),
            'image/height': tf.FixedLenFeature([], tf.int64),
            'image/width': tf.FixedLenFeature([], tf.int64)
        })
    image = tf.image.decode_jpeg(features["image/encoded"], 3)
    image = tf.reshape(image, [features['image/height'], features['image/width'], -1])
    if crop_and_resize:
        random_size = tf.random_uniform((1,), 0.9, 1)
        random_position = tf.random_uniform((1, 2), 0, 1-random_size)
        random_box = tf.concat([random_position, random_position + random_size], axis=-1)
    else:
        random_box = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
    image = tf.image.crop_and_resize(tf.expand_dims(image, 0), random_box, tf.constant([0]), image_output_shape)
    image = tf.squeeze(image)
    image = tf.image.random_flip_left_right(image)
    image = vgg19.preprocess_input(image, 'channels_last')
    image = tf.cast(image, tf.float32)
    image = tf.reshape(image, [image_output_shape[0], image_output_shape[1], 3])
    label = tf.cast([0.0], dtype=tf.float32) # unused
    return image


def create_dataset_from_records(file_pattern, parser, batch_size=4):
    filenames = tf.data.Dataset.list_files(file_pattern)
    dataset = filenames.apply(tf.data.experimental.shuffle_and_repeat(10))
    dataset = dataset.apply(tf.data.experimental.parallel_interleave(
        tf.data.TFRecordDataset, cycle_length=4))
    dataset = dataset.map(parser, num_parallel_calls=16).batch(batch_size, drop_remainder=True).prefetch(batch_size)
    return dataset


def input_fn(params):
    content_dataset = create_dataset_from_records(
        COCO_PATH, content_parser, 8*BATCH_SIZE)
    style_dataset = create_dataset_from_records(
        WIKI_PATH, content_parser, 8*BATCH_SIZE)

    content_images = content_dataset.make_one_shot_iterator().get_next()
    style_images = style_dataset.make_one_shot_iterator().get_next()
    features = {'content_images': content_images,
                'style_images': style_images}
    label = None
    return features, None


def input_predict_fn(params):
    content_dataset = create_dataset_from_records(
        COCO_PATH, lambda x: content_parser(x, False), 8*BATCH_SIZE)
    style_dataset = create_dataset_from_records(
        WIKI_PATH, lambda x: content_parser(x, False), 8*BATCH_SIZE)

    content_images = content_dataset.take(10).make_one_shot_iterator().get_next()
    style_images = style_dataset.take(10).make_one_shot_iterator().get_next()
    features = {'content_images': content_images,
                'style_images': style_images}
    label = None
    return features, None

model_fn = adain_style_transfer_model_fn



if True:
  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)
  strategy = tf.contrib.tpu.TPUDistributionStrategy(tpu_cluster_resolver)
  model_dir = 'gs://coco-tfrecords/model_dir/'
  model_dir = 'gs://tf-model-dir/style_transfer_adaptive_instance_like_dir/'

  run_config = tf.contrib.tpu.RunConfig(
      cluster=tpu_cluster_resolver,
      model_dir=model_dir,
      session_config=tf.ConfigProto(
            allow_soft_placement=True, log_device_placement=True),
     tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_shards),
  )

  # TPU-based estimator used for TRAIN and EVAL
  tpu_estimator_args = dict(
    model_fn=model_fn,
    params=params_estimator,
    config=run_config,
    train_batch_size=batch_size_tpu,
    eval_batch_size=batch_size_tpu,
    use_tpu=True)

  # CPU-based estimator used for PREDICT (generating images)
  cpu_estimator_args = dict(
    model_fn=model_fn,
    params=params_estimator,
    config=run_config,
    predict_batch_size=100,
    use_tpu=False)

  if warm_start_settings:
    tpu_estimator_args['warm_start_from'] = warm_start_settings
    cpu_estimator_args['warm_start_from'] = warm_start_settings

  est = tf.contrib.tpu.TPUEstimator(**tpu_estimator_args)

  cpu_est = tf.contrib.tpu.TPUEstimator(**cpu_estimator_args)

# !gsutil cp -r gs://coco-tfrecords/autoencoder_dir/ /content/drive/

# Test on GPU runtime

if False:
  model_dir = 'gs://coco-tfrecords/model_dir/'
  model_dir = 'gs://tf-model-dir/autoencoder_dir/'
  ws = tf.estimator.WarmStartSettings(
    ckpt_to_initialize_from="gs://coco-tfrecords/vgg19_encoder/weights.ckpt",
    vars_to_warm_start=vgg19_encoder_variables)

  # train_steps = int(1e4)
  train_steps_per_eval = int(1e3)
  iterations_per_loop = 50

  # CPU-based estimator used for PREDICT (generating images)
  cpu_est = tf.estimator.Estimator(
      model_fn=model_fn, model_dir=model_dir)

import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt

def show_images(image_batch, fig_size=24, columns=4):
    rows = (image_batch.shape[0] + 1) // (columns)
    fig = plt.figure(figsize = (fig_size, (fig_size // columns) * rows))
    gs = gridspec.GridSpec(rows, columns)
    for j in range(rows*columns):
        plt.subplot(gs[j])
        plt.axis("off")
        img_hwc = deprocess_image(image_batch[j])
        plt.imshow(img_hwc)

if False:
  sess = keras.backend.get_session()
  res = sess.run(dataset.make_one_shot_iterator().get_next())
  show_images(res[0][:, :, :, :, 1])
  # show_images(res[0][:, :, :, :, 0])

def debug_fn():
  graph = tf.get_default_graph()
  v = graph.get_tensor_by_name('Const_1:0')
  sess = tf.InteractiveSession()
  sess.run(v)

if False:
  res = cpu_est.predict(input_predict_fn)
  images = [next(res) for i in range(8)]
  images = [d['generated_images'] for d in images]
  show_images(np.stack(images))


metrics = cpu_est.evaluate(input_fn=input_predict_fn, steps=10)
tf.logging.info('Finished evaluating')
tf.logging.info(metrics)

if True:
  tf.gfile.MakeDirs(os.path.join(model_dir, 'generated_images'))

  current_step = estimator._load_global_step_from_checkpoint_dir(model_dir)   # pylint: disable=protected-access,line-too-long
  tf.logging.info('Starting training for %d steps, current step: %d' %
                  (1e5, current_step))
  while current_step < train_steps:
    next_checkpoint = min(current_step + train_steps_per_eval,
                          train_steps)
    next_checkpoint = int(next_checkpoint)
    # next_checkpoint = np.cast(next_checkpoint, np.float64)
    est.train(input_fn=input_fn,
              max_steps=next_checkpoint)
    current_step = next_checkpoint
    tf.logging.info('Finished training step %d' % current_step)

    if True:
      # Evaluate loss on test set
      metrics = cpu_est.evaluate(input_fn=input_predict_fn, steps=10)
      tf.logging.info('Finished evaluating')
      tf.logging.info(metrics)

    # Render some generated images
    # res = cpu_est.predict(input_predict_fn)
    # images = [next(res) for i in range(8)]
    # images = [d['generated_images'] for d in images]
    # show_images(np.stack(images))
    # generated_iter = cpu_est.predict(input_fn=input_predict_fn)
    # images = [p['generated_images'][:, :, :] for p in generated_iter]
    # assert len(images) == _NUM_VIZ_IMAGES
    # image_rows = [np.concatenate(images[i:i+10], axis=0)
    #              for i in range(0, _NUM_VIZ_IMAGES, 10)]
    # tiled_image = np.concatenate(image_rows, axis=1)

tf.logging.info('Finished generating images')

# reset_tf()
# last_ckpt = tf.train.latest_checkpoint('gs://coco-tfrecords/autoencoder_dir/')
# sess = tf.Session()
# new_saver = tf.train.import_meta_graph('gs://coco-tfrecords/autoencoder_dir/model.ckpt-11000.meta')
# new_saver.restore(sess, 'gs://coco-tfrecords/autoencoder_dir/model.ckpt-11000')
# cpu_est.latest_checkpoint()
#Â [cpu_est.get_variable_value(s) for s in vgg19_variables]

# sess = tf.get_default_session()
# vgg19_variables_tensor = filter(lambda v: v.name in vgg19_variables, tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)) 
# vgg19_variables_tensor = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='vgg19*')
# vgg19_variables_tensor = list(filter(lambda v: 'Adam' not in v.name, vgg19_variables_tensor))

# [v.name for v in vgg19_variables_tensor]

# saver = tf.train.Saver(vgg19_variables_tensor)
# saver.save(sess, 'gs://coco-tfrecords/autoencoder_dir/vgg19_variables.ckpt')

# import the inspect_checkpoint library
from tensorflow.python.tools import inspect_checkpoint as chkp

# print all tensors in checkpoint file
# chkp.print_tensors_in_checkpoint_file("gs://coco-tfrecords/autoencoder_dir/model.ckpt-11520", tensor_name='', all_tensors=True)
model_vars = est.get_variable_names()
model_vars = [s for s in model_vars if 'vgg19' in s and 'Adam' not in s]
